# Simple Neural Network (MNIST) â€” Following *Neural Networks and Deep Learning* by Michael Nielsen

This project implements a simple **feedforward neural network** trained via **stochastic gradient descent (SGD)** and **backpropagation**, following the principles and code structure presented in [*Neural Networks and Deep Learning*](http://neuralnetworksanddeeplearning.com/).
The network is trained on the **MNIST handwritten digits dataset**, achieving around **95% classification accuracy** on the test set after 30 epochs.

## Overview

The goal of this project is to build and train a fully connected neural network **from scratch using only NumPy**, without relying on high-level deep learning frameworks like TensorFlow or PyTorch.  

## Features

- Fully connected neural network architecture with arbitrary layer sizes  
- Sigmoid activation function  
- Backpropagation for computing gradients  
- Stochastic Gradient Descent (SGD) with mini-batches  
- Evaluation on MNIST test data  
- Easily extendable to other datasets or activation functions  

